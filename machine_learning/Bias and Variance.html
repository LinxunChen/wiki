<!DOCTYPE HTML>
<html>
  <head>
    <link rel="Stylesheet" type="text/css" href="/wiki/static/css/style.css">
    <link rel="Stylesheet" type="text/css" href="/wiki/static/css/tango.css">
    <link rel="shortcut icon" href="/wiki/favicon.ico" type="image/x-icon">
    <link rel="icon" href="/wiki/favicon.ico" type="image/x-icon">
    <link rel="alternate" type="application/atom+xml" href="atom.xml" title="Atom feed">
    <title>Bias and Variance - Wiki World</title>
    <meta name="keywords" content=""/>
    <meta name="description" content="linxunchen's wiki"/>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
          tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
        });
    </script>
    <script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
  </head>

  <body>
    <div id="container">
      
<div id="header">
  <div class="post-nav"><a href="/wiki/">Home</a>&nbsp;&#187;&nbsp;<a href="/wiki/#machine_learning">machine_learning</a>&nbsp;&#187;&nbsp;Bias and Variance
    <span class="updated">Updated&nbsp;
      2016-07-23 12:27:10
    </span></div>
</div>
<div class="clearfix"></div>

<div class="page_title">Bias and Variance</div>

  <p>就机器学习算法来说，其泛化误差可以分解为两部分，偏差（bias)和方差(variance)。这个可由下式（这里用到了概率论公式$D(X)=E(X^2)-E(X)^2$）导出。偏差指的是算法的期望预测与真实预测之间的偏差程度，反应了模型本身的拟合能力；方差度量了同等大小的训练集的变动导致学习性能的变化，刻画了数据扰动所导致的影响。</p>
<p><img alt="" src="http://oa5sa0jqw.bkt.clouddn.com/f9000a007cd7d0f8f421b287645ab35e.png" /></p>
<p>如下图所示，当模型越复杂时，拟合的程度就越高，模型的训练偏差就越小。但此时如果换一组数据可能模型的变化就会很大，即模型的方差很大。所以模型过于复杂的时候会导致过拟合。
当模型越简单时，即使我们再换一组数据，最后得出的学习器和之前的学习器的差别就不那么大，模型的方差很小。还是因为模型简单，所以偏差会很大。</p>
<p><img alt="" src="http://oa5sa0jqw.bkt.clouddn.com/2c75cea845d39d79bb32ced71078d3b3.png" /></p>
<p>也就是说，当我们训练一个模型时，偏差和方差都得照顾到，漏掉一个都不行。</p>
<p>对于Bagging算法来说，由于我们会并行地训练很多不同的分类器的目的就是降低这个方差(variance) ,因为采用了相互独立的基分类器多了以后，h的值自然就会靠近。所以对于每个基分类器来说，目标就是如何降低这个偏差（bias),所以我们会采用深度很深甚至不剪枝的决策树。</p>
<p>对于Boosting来说，每一步我们都会在上一轮的基础上更加拟合原数据，所以可以保证偏差（bias）,所以对于每个基分类器来说，问题就在于如何选择variance更小的分类器，即更简单的分类器，所以我们选择了深度很浅的决策树。</p>
<p>参考：</p>
<div class="hlcode"><pre><span class="mf">1.</span> <span class="n">https</span><span class="o">:</span><span class="c1">//www.zhihu.com/question/45487317/answer/99153174</span>
<span class="mf">2.</span> <span class="n">https</span><span class="o">:</span><span class="c1">//www.zhihu.com/question/27068705</span>
</pre></div>
    </div>
    <div id="footer">
      <span>
        <p>Copyright © 2017 linxunchen.
        Powered by <a href="http://simiki.org/" target="_blank">Simiki</a>.</p>
        <p>Site Generated 2017-01-06 00:06:57</p>
      </span>
    </div>
  </body>
</html>